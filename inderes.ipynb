{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65f28ef",
   "metadata": {},
   "source": [
    "## Inderes login & password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba73fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "# Edit secrets.json file with your login details\n",
    "with open('secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "    LOGIN_EMAIL = secrets['INDERES_LOGIN_EMAIL']\n",
    "    LOGIN_PASSWORD = secrets['INDERES_LOGIN_PASSWORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41cf3be",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db88bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import itertools\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77799f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urljoin\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "from http.client import IncompleteRead\n",
    "from lxml.html import fromstring, tostring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79787cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e675a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f3f8b",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e9731",
   "metadata": {},
   "source": [
    "### Web crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e5b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this with Redis or something similar\n",
    "class EventQueue(object):\n",
    "    \n",
    "    def __init__(self, cache, header):\n",
    "        self.cache = cache\n",
    "        self.header = header\n",
    "        self.last = None\n",
    "        # Read last pushed dictionary if the cache exists\n",
    "        if os.path.exists(self.cache):\n",
    "            with open(self.cache, 'r', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                row = None\n",
    "                for row in reader:\n",
    "                    pass\n",
    "                self.last = row\n",
    "    \n",
    "    def read(self):\n",
    "        with open(self.cache, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            data = [row for row in reader]\n",
    "        return(data)\n",
    "    \n",
    "    def push(self, rows):\n",
    "        first_push = not os.path.exists(self.cache)\n",
    "        with open(self.cache, 'a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, self.header) \n",
    "            if first_push:\n",
    "                writer.writeheader()\n",
    "            for row in rows:\n",
    "                writer.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7ddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use urllib to fetch a given website\n",
    "def download(url, num_retries=5, user_agent='wswp', charset='utf-8', proxy=None):\n",
    "    print('Downloading:', url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('User-agent', user_agent)\n",
    "    try:\n",
    "        if proxy:\n",
    "            proxy_support = urllib.request.ProxyHandler({'http': proxy})\n",
    "            opener = urllib.request.build_opener(proxy_support)\n",
    "            urllib.request.install_opener(opener)\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError, HTTPError, ContentTooShortError, IncompleteRead) as e:\n",
    "        if hasattr(e, 'reason'):\n",
    "            print('Download error:', e.reason)\n",
    "        else:\n",
    "            print('Download error: incomplete read')\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            #if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "            return download(url, num_retries - 1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e261d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all published company news that are newer than a given crawl date\n",
    "def crawl_frontpage(url, crawl_limit=None, crawl_last=None, wait_time=1):\n",
    "    datas = deque()\n",
    "    stop_crawl = False\n",
    "    for page in itertools.count(0):\n",
    "        html = download(\"{url}?page={page}\".format(url=url, page=page))\n",
    "        tree = fromstring(html)\n",
    "\n",
    "        path1 = '/div[@class=\"c-node-teaser__top_content\"]/div[@class=\"c-node-teaser__header\"]'\n",
    "        path2 = '/div[@class=\"c-node-teaser__publish-details\"]'\n",
    "        news = tree.xpath('//article')\n",
    "        for div in news:\n",
    "            data = {\n",
    "                'title' : div.xpath('.{base}/a/h3/text()'.format(base=path1))[0],\n",
    "                'href' : div.xpath('.{base}/a/@href'.format(base=path1))[0],\n",
    "                'type' : div.xpath('.{base}{details}/span[contains(@class,\"c-node-teaser__publish-detail--type\")]/text()'.format(base=path1, details=path2))[0],\n",
    "                'created' : div.xpath('.{base}{details}/span[contains(@class,\"c-node-teaser__publish-detail--date\")]/span/text()'.format(base=path1, details=path2))[0],\n",
    "                'stocks' : \":\".join(div.xpath('.{base}{details}/span[contains(@class,\"c-node-teaser__publish-detail--tags\")]/a/text()'.format(base=path1, details=path2))),\n",
    "                'pages' : \":\".join(div.xpath('.{base}{details}/span[contains(@class,\"c-node-teaser__publish-detail--tags\")]/a/@href'.format(base=path1, details=path2)))\n",
    "            }\n",
    "        \n",
    "            day_created = datetime.datetime.strptime(data['created'], '%d.%m.%Y %H:%M')\n",
    "            if not crawl_limit is None and day_created < crawl_limit or data == crawl_last:\n",
    "                stop_crawl = True\n",
    "                break\n",
    "            \n",
    "            datas.appendleft(data)\n",
    "\n",
    "        if len(news) == 0 or stop_crawl:\n",
    "            break\n",
    "\n",
    "        time.sleep(wait_time)\n",
    "    return(datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "515b8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before site update\n",
    "def download_report(url, download_path, num_waits=10):\n",
    "    report_pdf = os.path.join(download_path, '{}.pdf'.format(url))\n",
    "    if not os.path.exists(report_pdf):\n",
    "        try:\n",
    "            driver.get('https://inderes.fi/fi/{url}'.format(url=url))\n",
    "            time.sleep(1.0)\n",
    "            dl = driver.find_element(By.XPATH, '//span[@class=\"file\"]/a') #old website\n",
    "            file = dl.get_attribute('href')\n",
    "            #webdriver.ActionChains(driver).move_to_element(dl).click(dl).perform()\n",
    "            dl.click()\n",
    "            # Rename [file].pdf into corresponding [url].pdf\n",
    "            download_pdf = os.path.join(download_path, os.path.basename(file))\n",
    "            while not os.path.exists(download_pdf) and num_waits > 0:\n",
    "                time.sleep(0.5)\n",
    "                num_waits -= 1\n",
    "            os.rename(download_pdf, report_pdf)\n",
    "        except Exception as e:\n",
    "            print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d86d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After site update\n",
    "def download_report_new(url, download_path, num_waits=10):\n",
    "    report_pdf = os.path.join(download_path, '{}.pdf'.format(url))\n",
    "    if not os.path.exists(report_pdf):\n",
    "        try:\n",
    "            driver.get('https://inderes.fi/research/{url}'.format(url=url))\n",
    "            time.sleep(1.0)\n",
    "            dl = driver.find_element(By.CSS_SELECTOR, \"div[class^='ReportLink_downloadContainer'] > a\")\n",
    "            file = dl.get_attribute('href')\n",
    "            #webdriver.ActionChains(driver).move_to_element(dl).click(dl).perform()\n",
    "            dl.click()\n",
    "            # Rename [file].pdf into corresponding [url].pdf\n",
    "            download_pdf = os.path.join(download_path, \"{}.pdf\".format(os.path.basename(file)))\n",
    "            while not os.path.exists(download_pdf) and num_waits > 0:\n",
    "                time.sleep(0.5)\n",
    "                num_waits -= 1\n",
    "            os.rename(download_pdf, report_pdf)\n",
    "        except Exception as e:\n",
    "            print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ea2379",
   "metadata": {},
   "source": [
    "### Parsing the analysis PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d48e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of analysts to recognize in the file\n",
    "analysts = ['Antti Viljakainen','Petri Gostowski','Juha Kinnunen','Jesse Kinnunen','Erkki Vesola',\n",
    "            'Joni Grönqvist','Sauli Vilén','Sauli Vilen','Olli Koponen','Atte Riikola',\n",
    "            'Olli Vilppo','Joonas Korkiakoski','Antti Luiro','Petri Kajaani',\n",
    "            'Pauli Lohi','Thomas Westerholm','Matias Arola','Mikael Rautanen','Aapeli Pursimo',\n",
    "            'Antti Siltanen','Rauli Juva','Petri Aho','Rasmus Skand', 'Jari Honko',\n",
    "            'Anton Damstén', 'Roni Peuranheimo', 'Arttu Heikura', 'Frans-Mikael Rostedt']\n",
    "# Try to match the recommendation string, print ones not found\n",
    "pattern1 = re.compile(r'(RATING|SUOSITUS|RECOMMENDATION)\\s*:?\\s*(OSTA|LISÄÄ|VÄHENNÄ|MYY|BUY|ACCUMULATE|REDUCE|SELL)', re.IGNORECASE)\n",
    "pattern2 = re.compile(r'(Osta|Lisää|Vähennä|Myy|Pidä|Accumulate|Reduce)\\s*(\\(aik.|Edellinen:|Previous :)\\s*(Osta|Lisää|Vähennä|Myy|Pidä|Accumulate|Reduce)\\)', re.IGNORECASE)\n",
    "pattern3 = re.compile(r'\\(aik. (Osta|Lisää|Vähennä|Myy|Pidä)\\)\\s*(Osta|Lisää|Vähennä|Myy|Pidä)', re.IGNORECASE)\n",
    "# Read the recommendation from the found string\n",
    "x1 = re.compile(r'(RATING|SUOSITUS|RECOMMENDATION)\\s*:?\\s*', re.IGNORECASE)\n",
    "x2 = re.compile(r'\\s*(\\(aik.|Edellinen:|Previous :)\\s*(Osta|Lisää|Vähennä|Myy|Pidä|Accumulate|Reduce)\\)', re.IGNORECASE)\n",
    "x3 = re.compile(r'\\(aik. (Osta|Lisää|Vähennä|Myy|Pidä)\\)\\s*', re.IGNORECASE)\n",
    "\n",
    "def parse_pdf(report_pdf, download_path):\n",
    "    suositus = ''\n",
    "    analyytikko = ''\n",
    "    report = os.path.splitext(report_pdf)[0]\n",
    "    report_pdf = os.path.join(download_path, report_pdf)\n",
    "    # find recommendation from file contents\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(report_pdf)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            s = page.extract_text()\n",
    "            s = \" \".join(s.split())\n",
    "            for pattern in (pattern1, pattern2, pattern3):\n",
    "                for group in pattern.finditer(s):\n",
    "                    if suositus == '':\n",
    "                        suositus = s[group.start():(group.end())]\n",
    "            if analyytikko == '':\n",
    "                analyytikko = ','.join([analyst for analyst in analysts if analyst in s])\n",
    "            if i > 1:\n",
    "                break\n",
    "    except:\n",
    "        print(\"error:\", end=\"\")\n",
    "    # remove extras\n",
    "    for x in (x1, x2, x3):\n",
    "        suositus = x.sub('', suositus).lower()\n",
    "    return(report, suositus, analyytikko)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0a1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_stock(data, fix_path='reports_info'):\n",
    "    print('Missing stock identifier (fix: analysis_stock_missing.csv)')\n",
    "    \n",
    "    data['stock'].fillna('', inplace=True)\n",
    "    data_stock_missing_new = data[data['stock'] == ''].copy()\n",
    "    print('\\tMissing before fixes: {}'.format((data_stock_missing_new['stock'] == '').sum()))\n",
    "\n",
    "    # read fixes from file\n",
    "    fn_stock_missing = os.path.join(fix_path, 'analysis_stock_missing.csv')\n",
    "    if not os.path.exists(fn_stock_missing):\n",
    "        data_stock_missing_new.to_csv(fn_stock_missing, index=False)\n",
    "    else:\n",
    "        data_stock_missing = pd.read_csv(fn_stock_missing, index_col=False, parse_dates=[2])\n",
    "        data_stock_missing['stock'].fillna('', inplace=True)\n",
    "        data_stock_missing_add = data_stock_missing_new[~data_stock_missing_new['url'].isin(data_stock_missing['url'])]\n",
    "        data_stock_missing_new = pd.concat([data_stock_missing, data_stock_missing_add])\n",
    "        data_stock_missing_new.to_csv(fn_stock_missing, index=False)\n",
    "\n",
    "    print('\\tMissing after fixes: {}'.format((data_stock_missing_new['stock'] == '').sum()))\n",
    "    \n",
    "    return(pd.concat([data[data['stock'] != ''], data_stock_missing_new]).sort_values('created'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5368a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_names(urls):\n",
    "    url_to_name = {}\n",
    "    for url in urls:\n",
    "        driver.get('https://www.inderes.fi/fi/{url}'.format(url=url))\n",
    "        time.sleep(0.5)\n",
    "        name = driver.find_element(By.XPATH, '//span[@class=\"c-node-teaser__publish-detail c-node-teaser__publish-detail--tags\"]/a')\n",
    "        url_to_name[url] = name.text\n",
    "    return(url_to_name)\n",
    "\n",
    "name_to_id = {\n",
    "    'Affecto': 'affecto',\n",
    "    'Ahlstrom':'ahlstrom', \n",
    "    'Ahtium': 'ahtium', \n",
    "    'Amer': 'amer-sports', \n",
    "    'Biotie Therapies': 'biotie-therapies',\n",
    "    'Comptel': 'comptel', \n",
    "    'Elektrobit': 'bittium', \n",
    "    'GF Money': 'gf-money' ,\n",
    "    'Hoivatilat': 'hoivatilat',\n",
    "    'Huhtanen Capital': 'huhtanen-capital',\n",
    "    'Lemminkäinen': 'lemminkainen',\n",
    "    'Mallisalkku': 'inderes-mallisalkku',\n",
    "    'Okmetic': 'okmetic', \n",
    "    'Sponda':'sponda', \n",
    "    'Taaleri': 'taaleri',\n",
    "    'Technopolis': 'technopolis'    \n",
    "}\n",
    "\n",
    "def fix_missing_stock2(data, fix_path='reports_info'):\n",
    "    print('Missing stock identifier fi (fix: analysis_stock_missing_name.csv) ')\n",
    "    \n",
    "    data_stock_missing_new2 = data.loc[data['stock'] == 'fi'].copy()\n",
    "    print('\\tMissing before fixes: {}'.format((data_stock_missing_new2['stock'] == 'fi').sum()))\n",
    "\n",
    "    # read fixes from file\n",
    "    fn_stock_missing = os.path.join(fix_path, 'analysis_stock_missing_name.csv')\n",
    "    if not os.path.exists(fn_stock_missing):\n",
    "        url_to_name = fetch_names(data_stock_missing_new2['url'])\n",
    "        pd.Series(url_to_name).to_csv(fn_stock_missing, header=False)\n",
    "    else:\n",
    "        url_to_name = pd.read_csv(fn_stock_missing, names=['url', 'name'], header=None)\n",
    "        url_to_name = dict(url_to_name.set_index('url')['name'])\n",
    "        new_urls = data_stock_missing_new2.loc[~data_stock_missing_new2['url'].isin(set(url_to_name.keys())), 'url']\n",
    "        url_to_name_add = fetch_names(new_urls)\n",
    "        url_to_name = {**url_to_name, **url_to_name_add}\n",
    "        pd.Series(url_to_name).to_csv(fn_stock_missing, header=False)\n",
    "\n",
    "    data_stock_missing_new2['stock'] = data_stock_missing_new2['url'].map(url_to_name).map(name_to_id).fillna('')\n",
    "    print('\\tMissing after fixes: {}'.format((data_stock_missing_new2['stock'] == '').sum()))\n",
    "\n",
    "    missing_names = set(url_to_name.values()).difference(set(name_to_id.keys()))\n",
    "    print('\\tMissing name -> stock mappings :\\n\\t{}'.format(missing_names))\n",
    "\n",
    "    return(pd.concat([data[data['stock'] != 'fi'], data_stock_missing_new2]).sort_values('created'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f30b0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_map = {'osta':'osta', 'lisää':'lisää', 'vähennä':'vähennä', 'myy':'myy', 'pidä':'pidä', \n",
    "           'buy':'osta', 'accumulate':'lisää', 'reduce':'vähennä', 'sell':'myy', '-':'-'}\n",
    "\n",
    "def fix_missing_recommendation(data, fix_path='reports_info'):\n",
    "    print('Missing recommendation (fix: analysis_recommendation_missing.csv)')\n",
    "    \n",
    "    data['suositus'].fillna('', inplace=True)\n",
    "    data_rec_missing_new = data[data['suositus'] == ''].copy()\n",
    "    print('\\tMissing before fixes: {}'.format((data_rec_missing_new['suositus'] == '').sum()))\n",
    "\n",
    "    # read fixes from file\n",
    "    fn_rec_missing = os.path.join(fix_path, 'analysis_recommendation_missing.csv')\n",
    "    if not os.path.exists(fn_rec_missing):\n",
    "        data_rec_missing_new.to_csv(fn_rec_missing, index=False)\n",
    "    else:\n",
    "        data_rec_missing = pd.read_csv(fn_rec_missing, index_col=False, parse_dates=[2])\n",
    "        data_rec_missing['suositus'].fillna('', inplace=True)\n",
    "        data_rec_missing_add = data_rec_missing_new[~data_rec_missing_new['url'].isin(data_rec_missing['url'])]\n",
    "        data_rec_missing_new = pd.concat([data_rec_missing, data_rec_missing_add])\n",
    "        data_rec_missing_new.to_csv(fn_rec_missing, index=False)\n",
    "\n",
    "    print('\\tMissing after fixes: {}'.format((data_rec_missing_new['suositus'] == '').sum()))\n",
    "    \n",
    "    data = pd.concat([data[data['suositus'] != ''], data_rec_missing_new]).sort_values('created')\n",
    "    data['suositus'] = data['suositus'].map(rec_map).fillna('')\n",
    "\n",
    "    print('\\tMissing after mapping: {}'.format((data['suositus'] == '').sum()))\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9211124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_analyst(data, fix_path='reports_info'):\n",
    "    print('Missing analyst (fix: analysis_analyst_missing.csv)')\n",
    "    \n",
    "    data['analyytikko'].fillna('', inplace=True)\n",
    "    data_analyst_missing_new = data[data['analyytikko'] == ''].copy()\n",
    "    print('\\tMissing before fixes: {}'.format((data_analyst_missing_new['analyytikko'] == '').sum()))\n",
    "\n",
    "    # read fixes from file\n",
    "    fn_analyst_missing = os.path.join(fix_path, 'analysis_analyst_missing.csv')\n",
    "    if not os.path.exists(fn_analyst_missing):\n",
    "        data_analyst_missing_new.to_csv(fn_analyst_missing, index=False)\n",
    "    else:\n",
    "        data_analyst_missing = pd.read_csv(fn_analyst_missing, index_col=False, parse_dates=[2])\n",
    "        data_analyst_missing['analyytikko'].fillna('', inplace=True)\n",
    "        data_analyst_missing_add = data_analyst_missing_new[~data_analyst_missing_new['url'].isin(data_analyst_missing['url'])]\n",
    "        data_analyst_missing_new = pd.concat([data_analyst_missing, data_analyst_missing_add])\n",
    "        data_analyst_missing_new.to_csv(fn_analyst_missing, index=False)\n",
    "\n",
    "    print('\\tMissing after fixes: {}'.format((data_analyst_missing_new['analyytikko'] == '').sum()))\n",
    "    \n",
    "    return(pd.concat([data[data['analyytikko'] != ''], data_analyst_missing_new]).sort_values('created'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e1b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_random_analyst(data):\n",
    "    take_random = {}\n",
    "    multiple = 0\n",
    "    for i, row in data.iterrows():\n",
    "        analysts = row['analyytikko'].split(',')\n",
    "        take_random[row['url']] = np.random.choice(analysts)\n",
    "        if len(analysts) > 1:\n",
    "            multiple += 1\n",
    "    data['analyytikko'] = data['url'].map(take_random)\n",
    "    print('\\tTake random from multiple analysts {}/{}'.format(multiple, len(data)))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1973ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_end_followup(data, no_update=365):\n",
    "    cutoff = pd.Timestamp(datetime.datetime.now())-pd.Timedelta(no_update, 'D')\n",
    "    # Get stocks last recommendation, replace by - if not updated in 365 days (end of follow-up)\n",
    "    fix_last = data.groupby('stock', as_index=False)[['created', 'suositus']].last()\n",
    "    fix_last.loc[fix_last['created'] < cutoff, 'suositus'] = '-'\n",
    "    data = data.merge(fix_last, how='left', on=['stock', 'created'], suffixes=('', '_korjaa'))\n",
    "    end_followup = (~data['suositus_korjaa'].isnull() & (data['suositus_korjaa'] != data['suositus'])).sum()\n",
    "    data.loc[~data['suositus_korjaa'].isnull(), 'suositus'] = data['suositus_korjaa']\n",
    "    data.drop(columns='suositus_korjaa', inplace=True)\n",
    "    print('\\tFix end of followup {}/{}'.format(end_followup, len(data)))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ff3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace614f1",
   "metadata": {},
   "source": [
    "## Initialize selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91ed52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = os.path.join(os.getcwd(), 'reports')\n",
    "webdriver_path = 'chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "755579ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvil4\\AppData\\Local\\Temp\\ipykernel_18232\\2057988904.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(webdriver_path, options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {'download.default_directory' : download_path,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"plugins.always_open_pdf_externally\": True,\n",
    "    \"download.open_pdf_in_system_reader\": False,\n",
    "    \"profile.default_content_settings.popups\": 0}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "driver = webdriver.Chrome(webdriver_path, options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd63712",
   "metadata": {},
   "source": [
    "## Inderes login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7cff41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://classic.inderes.fi/fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7222788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_button = driver.find_element(\"id\", \"edit-openid-connect-client-keycloak-login\")\n",
    "login_button.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0acb4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element('id', 'username').send_keys(LOGIN_EMAIL)\n",
    "driver.find_element('id', 'password').send_keys(LOGIN_PASSWORD + Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bccbe7",
   "metadata": {},
   "source": [
    "## Download front page updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "482cbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl_limit = datetime.datetime.strptime('01.01.2013 00:00', '%d.%m.%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4292870",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = EventQueue(cache = 'frontpage.csv', \n",
    "                    header = ['title', 'href', 'type', 'created', 'stocks', 'pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6dff85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://classic.inderes.fi/fi/node/1\"\n",
    "datas = crawl_frontpage(url, crawl_last=events.last, crawl_limit= datetime.datetime.strptime(\"11.10.2023\", '%d.%m.%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adc49181",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.push(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87758448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ea948b5",
   "metadata": {},
   "source": [
    "## Download reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13df8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('frontpage.csv')\n",
    "# add base url and stock\n",
    "data['created'] = pd.to_datetime(data['created'], format='%d.%m.%Y %H:%M', errors='coerce')\n",
    "data['url'] = [url.split('/')[-1] for url in data['href']]\n",
    "data['stock'] = [url.split('/')[-1] for url in data['pages'].fillna('')]\n",
    "# only relevant\n",
    "data = data.loc[(data['created'] >= pd.Timestamp('2013-01-01')) & (data['type'] == 'Analyysi'), ['stock', 'url', 'created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfdea7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61547</th>\n",
       "      <td>koskisen</td>\n",
       "      <td>matala-arvostus-vaimentaa-yksittaiset-pettymykset</td>\n",
       "      <td>2023-11-20 08:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61557</th>\n",
       "      <td>bbs</td>\n",
       "      <td>rahoituskierroksen-pitaisi-kantaa-ce-merkintaa...</td>\n",
       "      <td>2023-11-21 07:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61559</th>\n",
       "      <td>administer</td>\n",
       "      <td>halvalla-lahtee</td>\n",
       "      <td>2023-11-21 07:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61576</th>\n",
       "      <td>srv</td>\n",
       "      <td>hyva-asema-markkinan-jalleen-vetaessa</td>\n",
       "      <td>2023-11-22 08:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61590</th>\n",
       "      <td>panostaja</td>\n",
       "      <td>paaomat-taytyy-saada-tuottamaan</td>\n",
       "      <td>2023-11-23 08:12:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock                                                url  \\\n",
       "61547    koskisen  matala-arvostus-vaimentaa-yksittaiset-pettymykset   \n",
       "61557         bbs  rahoituskierroksen-pitaisi-kantaa-ce-merkintaa...   \n",
       "61559  administer                                    halvalla-lahtee   \n",
       "61576         srv              hyva-asema-markkinan-jalleen-vetaessa   \n",
       "61590   panostaja                    paaomat-taytyy-saada-tuottamaan   \n",
       "\n",
       "                  created  \n",
       "61547 2023-11-20 08:36:00  \n",
       "61557 2023-11-21 07:10:00  \n",
       "61559 2023-11-21 07:59:00  \n",
       "61576 2023-11-22 08:02:00  \n",
       "61590 2023-11-23 08:12:00  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a27ba9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save .pdfs to download folder\n",
    "for i, url in enumerate(data['url']):\n",
    "    print(url, \"{}/{}\".format(i+1, len(data)))\n",
    "    download_report(url, download_path) # before site update\n",
    "    download_report_new(url, download_path) # after site update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46c0cf",
   "metadata": {},
   "source": [
    "## Create recommendation file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf475cde",
   "metadata": {},
   "source": [
    "### Parse reports to recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17a1ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suositukset = []\n",
    "for i, pdf in enumerate(os.listdir(download_path)):\n",
    "    print(pdf, \"{}/{}\".format(i+1, len(data)))\n",
    "    suositukset.append(parse_pdf(pdf, download_path))\n",
    "suositukset = pd.DataFrame(suositukset, columns=['url', 'suositus', 'analyytikko'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f1fd269",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(suositukset,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "58941c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>suositus</th>\n",
       "      <th>analyytikko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7036</th>\n",
       "      <td>koskisen</td>\n",
       "      <td>matala-arvostus-vaimentaa-yksittaiset-pettymykset</td>\n",
       "      <td>2023-11-20 08:36:00</td>\n",
       "      <td>lisää</td>\n",
       "      <td>Antti Viljakainen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>bbs</td>\n",
       "      <td>rahoituskierroksen-pitaisi-kantaa-ce-merkintaa...</td>\n",
       "      <td>2023-11-21 07:10:00</td>\n",
       "      <td>vähennä</td>\n",
       "      <td>Antti Siltanen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>administer</td>\n",
       "      <td>halvalla-lahtee</td>\n",
       "      <td>2023-11-21 07:59:00</td>\n",
       "      <td>osta</td>\n",
       "      <td>Juha Kinnunen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>srv</td>\n",
       "      <td>hyva-asema-markkinan-jalleen-vetaessa</td>\n",
       "      <td>2023-11-22 08:02:00</td>\n",
       "      <td>lisää</td>\n",
       "      <td>Olli Koponen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>panostaja</td>\n",
       "      <td>paaomat-taytyy-saada-tuottamaan</td>\n",
       "      <td>2023-11-23 08:12:00</td>\n",
       "      <td>lisää</td>\n",
       "      <td>Juha Kinnunen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stock                                                url  \\\n",
       "7036    koskisen  matala-arvostus-vaimentaa-yksittaiset-pettymykset   \n",
       "7037         bbs  rahoituskierroksen-pitaisi-kantaa-ce-merkintaa...   \n",
       "7038  administer                                    halvalla-lahtee   \n",
       "7039         srv              hyva-asema-markkinan-jalleen-vetaessa   \n",
       "7040   panostaja                    paaomat-taytyy-saada-tuottamaan   \n",
       "\n",
       "                 created suositus        analyytikko  \n",
       "7036 2023-11-20 08:36:00    lisää  Antti Viljakainen  \n",
       "7037 2023-11-21 07:10:00  vähennä     Antti Siltanen  \n",
       "7038 2023-11-21 07:59:00     osta      Juha Kinnunen  \n",
       "7039 2023-11-22 08:02:00    lisää       Olli Koponen  \n",
       "7040 2023-11-23 08:12:00    lisää      Juha Kinnunen  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_csv(os.path.join('reports_info', 'analysis_unfixed.csv'), index=False)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c459f",
   "metadata": {},
   "source": [
    "### Fix missing stock indicator (NA/fi) and recommendation (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8c33b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('reports_info', 'analysis_unfixed.csv'), parse_dates=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3105671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing stock identifier (fix: analysis_stock_missing.csv)\n",
      "\tMissing before fixes: 26\n",
      "\tMissing after fixes: 5\n"
     ]
    }
   ],
   "source": [
    "data = fix_missing_stock(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f5d7b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing stock identifier fi (fix: analysis_stock_missing_name.csv) \n",
      "\tMissing before fixes: 217\n",
      "\tMissing after fixes: 0\n",
      "\tMissing name -> stock mappings :\n",
      "\tset()\n"
     ]
    }
   ],
   "source": [
    "data = fix_missing_stock2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "705bd950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing recommendation (fix: analysis_recommendation_missing.csv)\n",
      "\tMissing before fixes: 545\n",
      "\tMissing after fixes: 176\n",
      "\tMissing after mapping: 176\n"
     ]
    }
   ],
   "source": [
    "data = fix_missing_recommendation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "794256be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing analyst (fix: analysis_analyst_missing.csv)\n",
      "\tMissing before fixes: 147\n",
      "\tMissing after fixes: 76\n"
     ]
    }
   ],
   "source": [
    "data = fix_missing_analyst(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d89925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing analyst (fix: analysis_analyst_missing.csv)\n",
      "\tMissing before fixes: 157\n",
      "\tMissing after fixes: 82\n"
     ]
    }
   ],
   "source": [
    "data = fix_missing_analyst(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bd57484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTake random from multiple analysts 860/7045\n"
     ]
    }
   ],
   "source": [
    "data = take_random_analyst(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7f50d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFix end of followup 28/7045\n"
     ]
    }
   ],
   "source": [
    "data = fix_end_followup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a53fac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['analyytikko'] == 'Sauli Vilén', 'analyytikko'] = 'Sauli Vilen'\n",
    "# these were renamed\n",
    "data.loc[data['stock'] == 'loudspring', 'stock'] = 'eagle-filters'\n",
    "data.loc[data['stock'] == 'nightingale', 'stock'] = 'nightingale-health'\n",
    "data.loc[data['stock'] == 'alexandria-group', 'stock'] = 'alexandria-pankkiiriliike'\n",
    "data.loc[data['stock'] == 'sievi-capital', 'stock'] = 'kh-group'\n",
    "data.loc[data['stock'] == 'soprano', 'stock'] = 'wetteri'\n",
    "data.loc[data['stock'] == 'lifa-air', 'stock'] = 'pallas-air'\n",
    "# Metso Minerals + Outotec = Metso Outotec fusion, Metso -> Neles renaming, Metso Outotec -> Metso renaming \n",
    "data.loc[data['stock'] == 'metso-outotec', 'stock'] = 'metso-0'\n",
    "# Fellow Finance + Evli Pankki = Alisa Pankki fusion, Evli new listing\n",
    "data.loc[(data['stock'] == 'fellow-pankki') & (data['created'] < pd.Timestamp('2022-04-02')), 'stock'] = 'fellow-finance' #FELLOWX\n",
    "data.loc[(data['stock'] == 'fellow-pankki') & (data['created'] >= pd.Timestamp('2022-04-02')), 'stock'] = 'alisa-pankki' #ALISA\n",
    "data.loc[(data['stock'] == 'evli') & (data['created'] < pd.Timestamp('2022-04-02')), 'stock'] = 'evli-pankki' #ALISA\n",
    "# Reading fixes may have created some NaNs\n",
    "data['stock'].fillna('', inplace=True)\n",
    "data['suositus'].fillna('', inplace=True)\n",
    "data['analyytikko'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a8f7",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f9f8f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "data = data[(data['stock'] != '') & (data['suositus'] != '')]\n",
    "data.to_csv('analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ce42c759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ceb74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30aea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa9f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc2f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20b2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df63c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375489af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c816190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9b2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inderes)",
   "language": "python",
   "name": "inderes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
